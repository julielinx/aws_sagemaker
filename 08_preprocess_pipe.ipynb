{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SM08: Preprocessing Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to preprocess the [Insurance Company Benchmark (COIL 2000) dataset](https://archive.ics.uci.edu/ml/datasets/Insurance+Company+Benchmark+%28COIL+2000%29) was developed in posts [SM07](). This notebook will turn that code into the script for the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update EC2 instance\n",
    "\n",
    "Writing the `preprocess.py` script is very similar to writing the `etl.py` script. The major difference is that I want to use a library that isn't installed by default and want to ensure package versions for several libraries. To do this I need to consider the pre-built EC2 instance configurations vs other options.\n",
    "\n",
    "AWS provides several different pre-built EC2 instance configurations. Unfortunately, there's always one package that needs to be updated to a specific version or isn't included by default. AWS generally recommends the following two solutions:\n",
    "\n",
    "- Use a `requirements.txt` (only available for estimator instances, not processor instances)\n",
    "- Create a custom EC2 image, load it to ECR (elastic container registry), and reference it in your pipeline\n",
    "\n",
    "### `requirements.txt`\n",
    "\n",
    "When first starting out, we don't want to have to figure out how to convert a transformer to an estimator. We just want to be able to run the python script and save the outputs to a designated S3 location. So, the `requirements.txt` is out. For information on how to do it, see the [Using thrid-party libraries](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#using-third-party-libraries) section in the documentation.\n",
    "\n",
    "### Custom image\n",
    "\n",
    "The directions to create a custom EC2 image generally involve going into another system, such as the AWS CLI. Our goal is to keep as much together in SageMaker as humanly possible. This rules out creating our own image. For information on how to create an image and load it to ECR, see the [Building your own algorithm container](https://sagemaker-examples.readthedocs.io/en/latest/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.html#Building-your-own-algorithm-container) section of the documentation or [Pushing a Docker image](https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html) in the ECR User Guide. \n",
    "\n",
    "*Fair warning*, it isn't recommended to create a docker container from within a docker container (which is what SageMaker Studio is). To create the container, you'll need to use the AWS CLI or a SageMaker instance (not Studio).\n",
    "\n",
    "### The solution\n",
    "\n",
    "Stackoverflow to the rescue. [This answer](https://stackoverflow.com/a/63925135) gave us the information we needed to simply install or update the specific packages we needed. The code is included directly in the python script and is easy to use. We update the code to be able to load or upgrade a package as necessary.\n",
    "\n",
    "If we get to the point that we frequently need a specific configuration, we'll want to further explore creating our own image to upload to ECR.\n",
    "\n",
    "The code to install or upgrade a package on the EC2 is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\n",
    "def upgrade(package):\n",
    "    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package, '--upgrade'])\n",
    "    \n",
    "upgrade('pandas==1.3.5')\n",
    "upgrade('numpy')\n",
    "install('category_encoders')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create subdirectories\n",
    "\n",
    "I decided to split the data into train, test, and validate in this step. The reason for this is that if the processing is based in any way on the data in the features (for example normalizing numeric values), this can cause data leakage. If I train and evaluate a model on data that was preprocessed all at the same time, the preprocessing - model process will never be evaluated on data it's never seen before. It will always be training on data that was preprocessed with the rest of the data. For more information on data leakage due to preprocessing see:\n",
    "\n",
    "- [Entry 17: Resampling](https://julielinx.github.io/blog/17_resampling/)\n",
    "- [Entry 20: Scikit-Learn Pipeline](https://julielinx.github.io/blog/20_sklearn_pipeline/)\n",
    "\n",
    "In splitting the data, I now need to save multiple datasets. In order to use them in the SageMaker pipeline, I've found it's best to save these under different names for easy referencing in different pipeline steps. To do this, I create subdirectories in the `/opt/ml/processing/output` folder. The code to create subdirectories is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    try:\n",
    "        os.makedirs(os.path.join(output_path, \"train\"))\n",
    "        os.makedirs(os.path.join(output_path, \"validate\"))\n",
    "        os.makedirs(os.path.join(output_path, \"test\"))\n",
    "        os.makedirs(os.path.join(output_path, 'encoder'))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "\n",
    "There are several options when splitting data. One of the most popular options is to use the `sklearn.model_selection.train_test_split` function. This is a very handy function for splitting data into train and test datasets. It allows me to specify the size of either the train or the test data, set a random state so that the split is reproducible, shuffle the data prior to splitting, and stratify as appropriate. However, it only splits the data into two - train and test.\n",
    "\n",
    "In order to get train, test, and validate datasets, I could use `train_test_split` twice or I could use `numpy.split`. Several examples in the [amazon-sagemaker-examples repo](https://github.com/aws/amazon-sagemaker-examples) use `numpy.split`, so I opted to use this in my code. The code to split the data is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    train_data, validation_data, test_data = np.split(\n",
    "        processed_df.sample(frac=1, random_state=1729),\n",
    "        [int(0.7 * len(processed_df)), int(0.9 * len(processed_df))],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save encoder\n",
    "\n",
    "The last of the new functionalities added to this `.py` script is to save the encoder.\n",
    "\n",
    "The encoder is the trained transformer that actually changes the data. In my case, it's the trained one hot encoder. The trained one hot encoder determines how the categorical values are turned into numeric features and ensures that this is reproducible regardless of whether new categories are added to subsequent observations.\n",
    "\n",
    "Once trained, the encoder needs to be saved. I use `joblib.dump` to do this. Just like the three datasets, I put the encoder in its own folder so I can easily reference it in the SageMaker pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    joblib.dump(encoder, os.path.join(output_path, 'encoder', encoder_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Script\n",
    "\n",
    "Now that I've covered the code for the additional functionality I need to make this script work, I can put it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\n",
    "def upgrade(package):\n",
    "    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package, '--upgrade'])\n",
    "    \n",
    "upgrade('pandas==1.3.5')\n",
    "upgrade('numpy')\n",
    "install('category_encoders')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_path = '/opt/ml/processing/input'\n",
    "    output_path = '/opt/ml/processing/output'\n",
    " \n",
    "    try:\n",
    "        os.makedirs(os.path.join(output_path, \"train\"))\n",
    "        os.makedirs(os.path.join(output_path, \"validate\"))\n",
    "        os.makedirs(os.path.join(output_path, \"test\"))\n",
    "        os.makedirs(os.path.join(output_path, 'encoder'))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    cat_cols = ['zip_agg Customer Subtype', 'zip_agg Customer main type']\n",
    "\n",
    "    ori_df = pd.read_csv(os.path.join(input_path, 'full_data.csv'))\n",
    "    df = pd.DataFrame(ori_df['Nbr mobile home policies']).merge(ori_df.drop('Nbr mobile home policies', axis=1), left_index=True, right_index=True)\n",
    "    print('Preprocessing data')\n",
    "    encoder = ce.OneHotEncoder(cols=cat_cols, use_cat_names=True, handle_missing='return_nan')\n",
    "\n",
    "    train_data, validation_data, test_data = np.split(\n",
    "        df.sample(frac=1, random_state=1729),\n",
    "        [int(0.7 * len(df)), int(0.9 * len(df))],)\n",
    "    \n",
    "    train_data = encoder.fit_transform(train_data)\n",
    "    validation_data = encoder.transform(validation_data)\n",
    "    test_data = encoder.transform(test_data)\n",
    "    \n",
    "    print('Saving dataframe')\n",
    "    train_data.to_csv(os.path.join(output_path, 'train', 'train_feats.csv'), index=False)\n",
    "    validation_data.to_csv(os.path.join(output_path, 'validate', 'validate_feats.csv'), index=False)\n",
    "    test_data.to_csv(os.path.join(output_path, 'test', 'test_feats.csv'), index=False)\n",
    "                              \n",
    "    print('Saving preprocessor joblib')\n",
    "    encoder_name = 'preprocessor.joblib'\n",
    "    joblib.dump(encoder, os.path.join(output_path, 'encoder', encoder_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Pipeline\n",
    "\n",
    "The last step is to write pipeline to run the `.py` script. The only new functionality I add to this pipeline is specifying the locations for multiple outputs.\n",
    "\n",
    "For simplicity, I reference the output saved to S3 from the ETL pipeline, I don't combine the two pipelines into a single pipeline at this stage. It's much faster to prototype and troubleshoot when working on a single step.\n",
    "\n",
    "If I combine steps before debugging, I have to wait for the first step to complete before the second one runs every single time. Just spinning up an instance on which to run a single step can take five minutes or more depending on the instance type. This time adds up really fast while debugging especially with pipelines that have many steps.\n",
    "\n",
    "Multistep pipelines will be covered in [Entry SM12: Multistep pipelines](). For a refresher on the foundations of SageMaker pipelines, see [Entry SM03: ETL Pipeline definition](). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:707031497630:pipeline/insexample/execution/bz7vxb9z8uag', sagemaker_session=<sagemaker.session.Session object at 0x7faddbf563d0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "import sagemaker.session\n",
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "session = sagemaker.session.Session()\n",
    "region = session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "prefix = '1_ins_dataset'\n",
    "pipeline_name = \"InsExample\"  # SageMaker Pipeline name\n",
    "model_package_group_name = \"Insurance Co Example\"  # Model name in model registry\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "input_uri = f's3://{bucket}/{prefix}/clean/full_data.csv'\n",
    "\n",
    "tags = [\n",
    "    {\"Key\": \"PLATFORM\", \"Value\": \"FO-ML\"},\n",
    "    {\"Key\": \"BUSINESS_REGION\", \"Value\": \"GLOBAL\"},\n",
    "    {\"Key\": \"BUSINESS_UNIT\", \"Value\": \"MOBILITY\"},\n",
    "    {\"Key\": \"CLIENT\", \"Value\": \"MULTI_TENANT\"}\n",
    "   ]\n",
    "\n",
    "# tags = [\n",
    "#     {\"Key\": \"DATASET\", \"Value\": \"InsCOIL\"},\n",
    "#     {\"Key\": \"SOURCE\", \"Value\": \"UCI\"}\n",
    "#    ]\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.t3.medium\")\n",
    "    \n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_uri\n",
    ")\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"ins-example-job\"\n",
    ")\n",
    "\n",
    "step_preprocess = ProcessingStep(\n",
    "    name=\"preprocess_data\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train\",\n",
    "            source=\"/opt/ml/processing/output/train\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3://{}\".format(bucket),\n",
    "                    prefix,\n",
    "                    'final',\n",
    "                    \"train\"\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"validate\",\n",
    "            source=\"/opt/ml/processing/output/validate\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3://{}\".format(bucket),\n",
    "                    prefix,\n",
    "                    'final',\n",
    "                    \"validate\"\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"test\",\n",
    "            source=\"/opt/ml/processing/output/test\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3://{}\".format(bucket),\n",
    "                    prefix,\n",
    "                    'final',\n",
    "                    \"test\"\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"encoder\",\n",
    "            source=\"/opt/ml/processing/output/encoder\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3://{}\".format(bucket),\n",
    "                    prefix,\n",
    "                    'final',\n",
    "                    'encoder'\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    code=\"preprocess.py\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        input_data,\n",
    "    ],\n",
    "    steps=[step_preprocess])\n",
    "\n",
    "pipeline.upsert(role_arn=role, tags=tags)\n",
    "\n",
    "pipeline.start(execution_display_name=\"InsPreprocess10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting category_encoders\n",
      "  Using cached category_encoders-2.5.1.post0-py2.py3-none-any.whl (72 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.8/site-packages (from category_encoders) (0.24.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.8/site-packages (from category_encoders) (1.7.1)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /opt/conda/lib/python3.8/site-packages (from category_encoders) (1.3.4)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.8/site-packages (from category_encoders) (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.8/site-packages (from category_encoders) (1.23.4)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.8/site-packages (from category_encoders) (0.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.0.5->category_encoders) (2021.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.20.0->category_encoders) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn>=0.20.0->category_encoders) (2.2.0)\n",
      "Collecting numpy>=1.14.0\n",
      "  Using cached numpy-1.22.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
      "Installing collected packages: numpy, category_encoders\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.23.4\n",
      "    Uninstalling numpy-1.23.4:\n",
      "      Successfully uninstalled numpy-1.23.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.22.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed category_encoders-2.5.1.post0 numpy-1.22.4\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -q -m pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "import joblib\n",
    "import os\n",
    "import sagemaker.session\n",
    "\n",
    "session = sagemaker.session.Session()\n",
    "region = session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "prefix = '1_ins_dataset'\n",
    "\n",
    "cat_cols = ['zip_agg Customer Subtype', 'zip_agg Customer main type']\n",
    "ori_df = pd.read_csv(f's3://{bucket}/{prefix}/clean/full_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip_agg Customer Subtype</th>\n",
       "      <th>zip_agg Number of houses</th>\n",
       "      <th>zip_agg Avg size household</th>\n",
       "      <th>zip_agg Avg age</th>\n",
       "      <th>zip_agg Customer main type</th>\n",
       "      <th>zip_agg Roman catholic</th>\n",
       "      <th>zip_agg Protestant</th>\n",
       "      <th>zip_agg Other religion</th>\n",
       "      <th>zip_agg No religion</th>\n",
       "      <th>zip_agg Married</th>\n",
       "      <th>...</th>\n",
       "      <th>Nbr private accident ins policies</th>\n",
       "      <th>Nbr family accidents ins policies</th>\n",
       "      <th>Nbr disability ins policies</th>\n",
       "      <th>Nbr fire policies</th>\n",
       "      <th>Nbr surfboard policies</th>\n",
       "      <th>Nbr boat policies</th>\n",
       "      <th>Nbr bicycle policies</th>\n",
       "      <th>Nbr property ins policies</th>\n",
       "      <th>Nbr ss ins policies</th>\n",
       "      <th>Nbr mobile home policies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lower class large families</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Family with grown ups</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mixed small town dwellers</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Family with grown ups</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mixed small town dwellers</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Family with grown ups</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Modern, complete families</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Average Family</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Large family farms</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Farmers</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     zip_agg Customer Subtype  zip_agg Number of houses  \\\n",
       "0  Lower class large families                         1   \n",
       "1   Mixed small town dwellers                         1   \n",
       "2   Mixed small town dwellers                         1   \n",
       "3   Modern, complete families                         1   \n",
       "4          Large family farms                         1   \n",
       "\n",
       "   zip_agg Avg size household  zip_agg Avg age zip_agg Customer main type  \\\n",
       "0                           3                2      Family with grown ups   \n",
       "1                           2                2      Family with grown ups   \n",
       "2                           2                2      Family with grown ups   \n",
       "3                           3                3             Average Family   \n",
       "4                           4                2                    Farmers   \n",
       "\n",
       "   zip_agg Roman catholic  zip_agg Protestant  zip_agg Other religion  \\\n",
       "0                       0                   5                       1   \n",
       "1                       1                   4                       1   \n",
       "2                       0                   4                       2   \n",
       "3                       2                   3                       2   \n",
       "4                       1                   4                       1   \n",
       "\n",
       "   zip_agg No religion  zip_agg Married  ...  \\\n",
       "0                    3                7  ...   \n",
       "1                    4                6  ...   \n",
       "2                    4                3  ...   \n",
       "3                    4                5  ...   \n",
       "4                    4                7  ...   \n",
       "\n",
       "   Nbr private accident ins policies  Nbr family accidents ins policies  \\\n",
       "0                                  0                                  0   \n",
       "1                                  0                                  0   \n",
       "2                                  0                                  0   \n",
       "3                                  0                                  0   \n",
       "4                                  0                                  0   \n",
       "\n",
       "   Nbr disability ins policies  Nbr fire policies  Nbr surfboard policies  \\\n",
       "0                            0                  1                       0   \n",
       "1                            0                  1                       0   \n",
       "2                            0                  1                       0   \n",
       "3                            0                  1                       0   \n",
       "4                            0                  1                       0   \n",
       "\n",
       "   Nbr boat policies  Nbr bicycle policies  Nbr property ins policies  \\\n",
       "0                  0                     0                          0   \n",
       "1                  0                     0                          0   \n",
       "2                  0                     0                          0   \n",
       "3                  0                     0                          0   \n",
       "4                  0                     0                          0   \n",
       "\n",
       "   Nbr ss ins policies  Nbr mobile home policies  \n",
       "0                    0                         0  \n",
       "1                    0                         0  \n",
       "2                    0                         0  \n",
       "3                    0                         0  \n",
       "4                    0                         0  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_path = '/opt/ml/processing/input'\n",
    "    output_path = '/opt/ml/processing/output'\n",
    " \n",
    "    try:\n",
    "        os.makedirs(os.path.join(output_path, \"train\"))\n",
    "        os.makedirs(os.path.join(output_path, \"validate\"))\n",
    "        os.makedirs(os.path.join(output_path, \"test\"))\n",
    "        os.makedirs(os.path.join(output_path, 'encoder'))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    cat_cols = ['zip_agg Customer Subtype', 'zip_agg Customer main type']\n",
    "\n",
    "    ori_df = pd.read_csv(os.path.join(input_path, 'full_data.csv'))\n",
    "    df = ori_df['Nbr mobile home policies'].merge(ori_df.drop('Nbr mobile home policies', axis=1), left_index=True, right_index=True)\n",
    "    print('Preprocessing data')\n",
    "    encoder = ce.OneHotEncoder(cols=cat_cols, use_cat_names=True, handle_missing='return_nan')\n",
    "\n",
    "    train_data, validation_data, test_data = np.split(\n",
    "        df.sample(frac=1, random_state=1729),\n",
    "        [int(0.7 * len(df)), int(0.9 * len(df))],)\n",
    "    \n",
    "    train_data = encoder.fit_transform(train_data)\n",
    "    validation_data = encoder.transform(validation_data)\n",
    "    \n",
    "    print('Saving dataframe')\n",
    "    train_data.to_csv(os.path.join(output_path, 'train', 'train_feats.csv'))\n",
    "    validation_data.to_csv(os.path.join(output_path, 'validate', 'validate_feats.csv'))\n",
    "    test_data.to_csv(os.path.join(output_path, 'test', 'test_feats.csv'))\n",
    "                              \n",
    "    print('Saving preprocessor joblib')\n",
    "    encoder_name = 'preprocessor.joblib'\n",
    "    joblib.dump(encoder, os.path.join(output_path, 'encoder', encoder_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
